{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73db6414",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Probability of (An BC) 15: 0.08333333333333333\n"
     ]
    }
   ],
   "source": [
    "A=(5/36) #(1,5), (2,4), (3,3), (4,2), (5,1) \n",
    "B=(6/36) #(1,2), (2,1),(2,4),(4,2), (3,6), (6,3) \n",
    "BC=(3/5) #(1,5), (3,3), (5,1)\n",
    "\n",
    "print(\"Probability of (An BC) 15:\",A*BC)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d5fee1a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.7\n"
     ]
    }
   ],
   "source": [
    "E1=float(2/5)\n",
    "\n",
    "E2=float (3/4)\n",
    "\n",
    "E3=float (2/3) \n",
    "E3bE2=float(4/5)\n",
    "\n",
    "E1bE2E3=float(1/2) \n",
    "print (E1+E3bE2*E2-E1bE2E3*E3bE2*E2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d337948",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The probabality that at least one will contain an error is: 0.030786545267269205\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "def fact(k):\n",
    "    if k==0 :return 1\n",
    "    return k*fact(k-1)\n",
    "def getpoisson (mu,k):\n",
    "    temp=math.pow(mu,0)*math.pow(math.e,-mu)\n",
    "    temp=temp/fact(k)\n",
    "    return temp\n",
    "\n",
    "p=905/289411 \n",
    "n=10\n",
    "\n",
    "mu=n*p\n",
    "\n",
    "val=getpoisson(mu,0) \n",
    "ans=1-val\n",
    "\n",
    "print(\"The probabality that at least one will contain an error is:\", ans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4412b4fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1c2c254",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"C:\\Users\\Sai Teja\\Desktop\\python\\csv files\\weatherAUS.csv\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6abcbbe4",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (Temp/ipykernel_24992/1351916434.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"C:\\Users\\Sai Teja\\AppData\\Local\\Temp/ipykernel_24992/1351916434.py\"\u001b[1;36m, line \u001b[1;32m3\u001b[0m\n\u001b[1;33m    steps followed\u001b[0m\n\u001b[1;37m          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "#rain-1 as 1.0 (yes) and 0.0 (0)\n",
    "\n",
    "steps followed\n",
    "\n",
    "importing libraries & dataset\n",
    "\n",
    "• Data analysis\n",
    "\n",
    "• Data visualisation & cleaning\n",
    "\n",
    "• DAta preprocessing\n",
    "\n",
    "• Detecting outliers\n",
    "\n",
    ". Model building\n",
    "\n",
    "compiling ANN\n",
    "\n",
    "• Training ANN\n",
    "\n",
    "• Training data plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "92b3bc49",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'keras'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_24992/1383604394.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodel_selection\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mtrain_test_split\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mseaborn\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0msns\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlayers\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mDense\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mBatchNormalization\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mDropout\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLSTM\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmodels\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mSequential\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0mkeras\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mutils\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mto_categorical\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'keras'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import datetime\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "import seaborn as sns\n",
    "from keras.layers import Dense, BatchNormalization, Dropout, LSTM\n",
    "from keras.models import Sequential\n",
    "from keras.utils import to_categorical\n",
    "from keras.optimizers import Adam\n",
    "from tensorflow.keras import regularizers\n",
    "from sklearn.metrics import precision_score, recall_score, confusion_matrix, classification_report, accuracy_score, f1_score\n",
    "from keras import callbacks\n",
    "\n",
    "np.random.seed(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52698912",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"../input/weather-dataset-rattle-package/weatherAUS.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f82ce11",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58902a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "col_names df.columns\n",
    "\n",
    "col_names\n",
    "\n",
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b9e1d2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()\n",
    "\n",
    "df['RainTomorrow'].isnull().sum()\n",
    "df['RainTomorrow'].nunique()\n",
    "df['RainTomorrow'].unique()\n",
    "df['RainTomorrow'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce30f227",
   "metadata": {},
   "outputs": [],
   "source": [
    "#first of all let us evaluate the target and find out if our data is imbalanced or not\n",
    "cols= [\"#C2C4E2\",\"#EED4E5\"]\n",
    "sns.countplot(x= df[\"RainTomorrow\"], palette= cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e918bb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Correlation amongst numeric attributes\n",
    "corrmat = df.corr()\n",
    "cmap = sns.diverging_palette(260,-10,s=50, l=75, n=6, as_cmap=True)\n",
    "plt.subplots(figsize=(18,18))\n",
    "sns.heatmap(corrmat,cmap= cmap,annot=True, square=True)\n",
    "\n",
    "#Parsing datetime\n",
    "#exploring the length of date objects\n",
    "lengths = df[\"Date\"].str.len()\n",
    "lengths.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3aa139e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#There don't seem to be any error in dates so parsing values into datetime\n",
    "df['Date']= pd.to_datetime(df[\"Date\"])\n",
    "#Creating a collumn of year\n",
    "df['year'] = df.Date.dt.year\n",
    "\n",
    "# function to encode datetime into cyclic parameters. \n",
    "#As I am planning to use this data in a neural network I prefer the months and days in a cyclic continuous feature. \n",
    "\n",
    "def encode(df, col, max_val):\n",
    "    df[col + '_sin'] = np.sin(2 * np.pi * df[col]/max_val)\n",
    "    df[col + '_cos'] = np.cos(2 * np.pi * df[col]/max_val)\n",
    "    return data\n",
    "\n",
    "df['month'] = df.Date.dt.month\n",
    "df = encode(df, 'month', 12)\n",
    "\n",
    "df['day'] = df.Date.dt.day\n",
    "df = encode(df, 'day', 31)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074e34a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of categorical variables\n",
    "s = (data.dtypes == \"object\")\n",
    "object_cols = list(s[s].index)\n",
    "\n",
    "print(\"Categorical variables:\")\n",
    "print(object_cols)\n",
    "\n",
    "# Missing values in categorical variables\n",
    "\n",
    "for i in object_cols:\n",
    "    print(i, data[i].isnull().sum())\n",
    "    \n",
    "#Filling missing values with mode of the column in value\n",
    "\n",
    "for i in object_cols:\n",
    "    data[i].fillna(data[i].mode()[0], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02b41da4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get list of neumeric variables\n",
    "t = (data.dtypes == \"float64\")\n",
    "num_cols = list(t[t].index)\n",
    "\n",
    "print(\"Neumeric variables:\")\n",
    "print(num_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cef6d1f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Missing values in numeric variables\n",
    "\n",
    "for i in num_cols:\n",
    "    print(i, data[i].isnull().sum())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3361aca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filling missing values with median of the column in value\n",
    "\n",
    "for i in num_cols:\n",
    "    data[i].fillna(data[i].median(), inplace=True)\n",
    "    \n",
    "data.info()\n",
    "\n",
    "# Apply label encoder to each column with categorical data\n",
    "label_encoder = LabelEncoder()\n",
    "for i in object_cols:\n",
    "    data[i] = label_encoder.fit_transform(data[i])\n",
    "    \n",
    "data.info()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7655a55e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prepairing attributes of scale data\n",
    "\n",
    "features = data.drop(['RainTomorrow', 'Date','day', 'month'], axis=1) # dropping target and extra columns\n",
    "\n",
    "target = data['RainTomorrow']\n",
    "\n",
    "#Set up a standard scaler for the features\n",
    "col_names = list(features.columns)\n",
    "s_scaler = preprocessing.StandardScaler()\n",
    "features = s_scaler.fit_transform(features)\n",
    "features = pd.DataFrame(features, columns=col_names) \n",
    "\n",
    "features.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57c996fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Detecting outliers\n",
    "#looking at the scaled features\n",
    "colours = [\"#D0DBEE\", \"#C2C4E2\", \"#EED4E5\", \"#D1E6DC\", \"#BDE2E2\"]\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.boxenplot(data = features,palette = colours)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f4e66dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#full data for \n",
    "features[\"RainTomorrow\"] = target\n",
    "\n",
    "#Dropping with outlier\n",
    "\n",
    "features = features[(features[\"MinTemp\"]<2.3)&(features[\"MinTemp\"]>-2.3)]\n",
    "features = features[(features[\"MaxTemp\"]<2.3)&(features[\"MaxTemp\"]>-2)]\n",
    "features = features[(features[\"Rainfall\"]<4.5)]\n",
    "features = features[(features[\"Evaporation\"]<2.8)]\n",
    "features = features[(features[\"Sunshine\"]<2.1)]\n",
    "features = features[(features[\"WindGustSpeed\"]<4)&(features[\"WindGustSpeed\"]>-4)]\n",
    "features = features[(features[\"WindSpeed9am\"]<4)]\n",
    "features = features[(features[\"WindSpeed3pm\"]<2.5)]\n",
    "features = features[(features[\"Humidity9am\"]>-3)]\n",
    "features = features[(features[\"Humidity3pm\"]>-2.2)]\n",
    "features = features[(features[\"Pressure9am\"]< 2)&(features[\"Pressure9am\"]>-2.7)]\n",
    "features = features[(features[\"Pressure3pm\"]< 2)&(features[\"Pressure3pm\"]>-2.7)]\n",
    "features = features[(features[\"Cloud9am\"]<1.8)]\n",
    "features = features[(features[\"Cloud3pm\"]<2)]\n",
    "features = features[(features[\"Temp9am\"]<2.3)&(features[\"Temp9am\"]>-2)]\n",
    "features = features[(features[\"Temp3pm\"]<2.3)&(features[\"Temp3pm\"]>-2)]\n",
    "\n",
    "\n",
    "features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a135c898",
   "metadata": {},
   "outputs": [],
   "source": [
    "#looking at the scaled features without outliers\n",
    "\n",
    "plt.figure(figsize=(20,10))\n",
    "sns.boxenplot(data = features,palette = colours)\n",
    "plt.xticks(rotation=90)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfecc48b",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = features.drop([\"RainTomorrow\"], axis=1)\n",
    "y = features[\"RainTomorrow\"]\n",
    "\n",
    "# Splitting test and training sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d7b3cbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Early stopping\n",
    "early_stopping = callbacks.EarlyStopping(\n",
    "    min_delta=0.001, # minimium amount of change to count as an improvement\n",
    "    patience=20, # how many epochs to wait before stopping\n",
    "    restore_best_weights=True,\n",
    ")\n",
    "\n",
    "# Initialising the NN\n",
    "model = Sequential()\n",
    "\n",
    "# layers\n",
    "\n",
    "model.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu', input_dim = 26))\n",
    "model.add(Dense(units = 32, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dense(units = 16, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dropout(0.25))\n",
    "model.add(Dense(units = 8, kernel_initializer = 'uniform', activation = 'relu'))\n",
    "model.add(Dropout(0.5))\n",
    "model.add(Dense(units = 1, kernel_initializer = 'uniform', activation = 'sigmoid'))\n",
    "\n",
    "# Compiling the ANN\n",
    "opt = Adam(learning_rate=0.00009)\n",
    "model.compile(optimizer = opt, loss = 'binary_crossentropy', metrics = ['accuracy'])\n",
    "\n",
    "# Train the ANN\n",
    "history = model.fit(X_train, y_train, batch_size = 32, epochs = 150, callbacks=[early_stopping], validation_split=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2765b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "plt.plot(history_df.loc[:, ['loss']], \"#BDE2E2\", label='Training loss')\n",
    "plt.plot(history_df.loc[:, ['val_loss']],\"#C2C4E2\", label='Validation loss')\n",
    "plt.title('Training and Validation loss')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend(loc=\"best\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ea605b",
   "metadata": {},
   "outputs": [],
   "source": [
    "history_df = pd.DataFrame(history.history)\n",
    "\n",
    "plt.plot(history_df.loc[:, ['accuracy']], \"#BDE2E2\", label='Training accuracy')\n",
    "plt.plot(history_df.loc[:, ['val_accuracy']], \"#C2C4E2\", label='Validation accuracy')\n",
    "\n",
    "plt.title('Training and Validation accuracy')\n",
    "plt.xlabel('Epochs')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0427391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Predicting the test set results\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = (y_pred > 0.5)\n",
    "# confusion matrix\n",
    "cmap1 = sns.diverging_palette(260,-10,s=50, l=75, n=5, as_cmap=True)\n",
    "plt.subplots(figsize=(12,8))\n",
    "cf_matrix = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cf_matrix/np.sum(cf_matrix), cmap = cmap1, annot = True, annot_kws = {'size':15})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
